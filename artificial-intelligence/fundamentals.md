---
title: Les Fondamentaux
description: 
published: true
date: 2026-01-10T21:55:33.268Z
tags: 
editor: markdown
dateCreated: 2025-12-27T12:34:46.295Z
---

# Les Fondamentaux

> üéØ **Objectifs de ce chapitre**
> 
> √Ä la fin de ce chapitre, tu seras capable de :
> - Comprendre ce qu'est r√©ellement l'IA (et ce qu'elle n'est pas !)
> - Diff√©rencier IA, Machine Learning et Deep Learning
> - Expliquer comment fonctionne un LLM comme ChatGPT √† tes coll√®gues
> - Retracer les grandes √©tapes de l'histoire de l'IA
> - Choisir le bon mod√®le pour tes projets
   
![ai-ml-ds.png](/ai_ml/ai-ml-ds.png "Infographie par Jen Looper de Microsoft"){.align-center}
   
Tu as s√ªrement entendu parler d'Intelligence Artificielle partout ces derniers temps. √Ä la t√©l√©, sur les r√©seaux sociaux, au bureau... Tout le monde en parle, mais combien de personnes savent vraiment ce que c'est ?

   Bonne nouvelle : √† la fin de ce cours, **toi**, tu sauras. Et tu pourras m√™me l'expliquer √† ta grand-m√®re (enfin, si elle est curieuse üòÑ).


## Qu'est ce que l'IA ?


> Imaginons que tu veuilles cr√©er un robot qui range ta chambre. Tu pourrais √©crire des milliers de lignes de code pour chaque situation possible : "si chaussette par terre, alors ramasser et mettre dans panier". Fastidieux, non ?
> 
>    L'**Intelligence Artificielle**, c'est justement l'id√©e de cr√©er des programmes capables de **s'adapter** et de **r√©soudre des probl√®mes** sans qu'on ait besoin de tout leur expliquer dans les moindres d√©tails.
{.is-info}


> üí° **D√©finition simple**
> L'Intelligence Artificielle (IA), c'est la capacit√© d'une machine ou d'un programme √† imiter l'intelligence humaine pour r√©aliser des t√¢ches complexes.
{.is-success}


>  En gros, au lieu de dire √† l'ordinateur exactement quoi faire dans chaque cas, on lui donne les moyens d'**apprendre** et de **s'adapter**. C'est un changement de philosophie radical !
{.is-info}


## Qu'est ce que le Machine Learning ?

> OK, l'IA c'est bien joli, mais concr√®tement, comment on fait pour qu'une machine "apprenne" ?
> 
>    C'est l√† qu'entre en sc√®ne le **Machine Learning** (ou Apprentissage Automatique en bon fran√ßais).
{.is-info}


> üí° **D√©finition simple**
> Le **"Machine Learning"** (Apprentissage Automatique) est un sous-domaine de l'IA o√π la machine apprend √† r√©soudre des t√¢ches en analysant des donn√©es, plut√¥t qu'en suivant des r√®gles dict√©es ligne par ligne par un d√©veloppeur.
{.is-success}

### Une analogie pour bien comprendre

   Imagine que tu veuilles apprendre √† un enfant √† reconna√Ætre un chat. Tu as deux options :

   **Option 1 : La m√©thode classique (programmation traditionnelle)**
   Tu lui donnes une liste de r√®gles :
   - "Un chat a 4 pattes"
   - "Un chat a des moustaches"
   - "Un chat fait miaou"
   - "Un chat a une queue"
   - ...

   Le probl√®me ? Un chien aussi a 4 pattes et une queue. Et certains chats n'ont pas de queue (comme le Manx). Ta liste de r√®gles va vite devenir un cauchemar √† maintenir !

   **Option 2 : La m√©thode Machine Learning**
   Tu montres √† l'enfant des **milliers de photos de chats** en lui disant "√ßa, c'est un chat". √Ä force, son cerveau va naturellement rep√©rer les patterns qui font qu'un chat est un chat.

   Le Machine Learning, c'est exactement √ßa : on montre des **tonnes d'exemples** √† la machine, et elle apprend toute seule √† reconna√Ætre les patterns.

   > ‚ÑπÔ∏è **Bon √† savoir**
   > 
   > Le Machine Learning n√©cessite beaucoup de donn√©es pour fonctionner. C'est pour √ßa qu'on parle souvent de "Big Data" dans le m√™me souffle. Plus tu as de donn√©es de qualit√©, meilleur sera ton mod√®le !

## Qu'est ce que le Deep Learning ?

> üí° **D√©finition simple**
> Le **Deep Learning** (Apprentissage Profond) est une sous-cat√©gorie du Machine Learning qui s'inspire de la structure du cerveau humain. Il utilise des "r√©seaux de neurones artificiels" compos√©s de nombreuses couches superpos√©es pour apprendre des donn√©es tr√®s complexes (images, son, texte).
{.is-success}

### Pourquoi "Deep" (Profond) ?

   Le mot "profond" fait r√©f√©rence au nombre de **couches** dans le r√©seau de neurones. Plus il y a de couches, plus le r√©seau est "profond", et plus il peut apprendre des choses complexes.

   Pense √† une usine avec plusieurs √©tages :
   - **√âtage 1** : On d√©tecte des formes simples (lignes, courbes)
   - **√âtage 2** : On combine ces formes en motifs (yeux, oreilles)
   - **√âtage 3** : On assemble les motifs en concepts (visage de chat)
   - **√âtage 4** : On reconna√Æt l'objet final ("C'est F√©lix le chat !")

   Chaque couche apprend quelque chose de plus abstrait que la pr√©c√©dente. C'est cette profondeur qui permet au Deep Learning de comprendre des choses aussi complexes que le langage humain ou la reconnaissance faciale.


## Qu'est ce qu'un LLM ?

   Maintenant qu'on a pos√© les bases, parlons de ce qui fait le buzz : les **LLM** (Large Language Models, ou Grands Mod√®les de Langage).

   ChatGPT, Claude, Gemini, Mistral... Tous ces assistants IA que tu utilises peut-√™tre d√©j√† sont des LLM.

> üí° **D√©finition simple**
> Un **LLM** (Large Language Model) est un mod√®le de langage capable de g√©n√©rer du texte √† partir d'un prompt donn√©. Il est bas√© sur du Deep Learning et entra√Æn√© sur une immense partie d'Internet, avec pour objectif principal de pr√©dire le mot suivant le plus probable dans une phrase.
{.is-success}

### Attends... "Pr√©dire le mot suivant" ? C'est tout ?

Eh oui ! Aussi impressionnants qu'ils paraissent, les LLM sont fondamentalement des **machines √† pr√©dire le mot suivant**. C'est leur seule mission dans la vie.

Mais attention, ne sous-estime pas cette t√¢che apparemment simple. Pour pr√©dire correctement le mot suivant, le mod√®le doit comprendre :
   - La grammaire
   - Le contexte
   - Les concepts
   - Les relations logiques
   - Et bien plus encore...


### Comment √ßa marche concr√®tement ?
![ai.fundamentals.token-guessing.jpg](/ai_ml/ai.fundamentals.token-guessing.jpg){.align-center}




![ai.fundamentals.tokenization.jpg](/ai_ml/ai.fundamentals.tokenization.jpg){.align-center}

> Le Mod√®le ne voit pas vraiment des "mots". Il voit des s√©quences de **tokens** (des morceaux de mots). Mais pour simplifier, restons avec les mots.
{.is-success}


Prenons l'exemple du Prompt : 
> Le chat mange ‚Üí [Le], [chat], [mange]

Le mod√®le calcule les probabilit√©s du mot suivant (Token) :
   | Mot suivant | Probabilit√© |
   |-------------|-------------|
   | des croquettes | 42% ‚úÖ |
   | une souris | 25% |
   | du poisson | 15% |
   | sa p√¢t√©e | 10% |
   | une pizza | 0.01% üçï |

Le mod√®le choisit g√©n√©ralement le mot le plus probable ("des croquettes"). Et ensuite ? Il recommence ! Il prend "Le chat mange des croquettes" et pr√©dit le mot suivant. Et ainsi de suite... c'est ce que l'on appelle l'**Auto-r√©gression**.

> ‚ö†Ô∏è **Attention !**
> 
> Un LLM est une **machine statistique**, pas une encyclop√©die. Il cherche le "probable", pas le "vrai". C'est pour √ßa qu'il peut parfois inventer des informations avec un aplomb d√©concertant !
> 
> Ce ph√©nom√®ne s'appelle l'**hallucination**. Le mod√®le ne "sait" pas qu'il dit des b√™tises. Il g√©n√®re simplement la suite de texte la plus probable selon ses calculs. Si cette suite probable est fausse... tant pis pour lui (et pour toi si tu ne v√©rifies pas) !
{.is-warning}

>  Tu as peut-√™tre entendu parler de la **temp√©rature** dans le contexte des LLM. C'est un param√®tre qui contr√¥le le niveau de "prise de risque" du mod√®le.

   | Temp√©rature | Comportement |
   |-------------|--------------|
   | **0** (basse) | Le mod√®le choisit toujours le mot le plus probable. R√©ponses pr√©visibles et "s√ªres". |
   | **0.7-0.8** (moyenne) | Un bon √©quilibre entre cr√©ativit√© et coh√©rence. |
   | **1.0+** (haute) | Le mod√®le prend des risques, peut choisir des mots moins probables. Plus cr√©atif, mais aussi plus susceptible de partir dans tous les sens ! |

Pour reprendre notre exemple du chat :
- Si la temp√©rature est √† 0, l'IA choisira toujours "des croquettes" (le plus probable).
- Si la temp√©rature est √©lev√©e (ex: 0.8), l'IA prendra des risques et pourra choisir "sa p√¢t√©e" ou m√™me "le canap√©" pour √™tre plus cr√©ative.


    
## Histoire de l'IA

> L'IA n'est pas n√©e avec ChatGPT en 2022. Son histoire remonte √† plus de 70 ans, avec des hauts spectaculaires et des bas tout aussi impressionnants.
{.is-info}


![ai.history_1.png](/ai_ml/ai.history_1.png)
![ai.history_2.png](/ai_ml/ai.history_2.png)

### 1950 : Des machines qui pensent

Tout commence avec un g√©nie britannique : **Alan Turing**.
> 
> üí° **Qui est Alan Turing ?**
> 
> √âlu scientifique du 20√®me si√®cle en 2019, Alan Turing est consid√©r√© comme le p√®re de l'IA et de l'informatique th√©orique. Il a formalis√© le concept d'algorithme avec sa "Machine de Turing" (1936) et jou√© un r√¥le crucial dans le d√©cryptage de la machine Enigma pendant la Seconde Guerre Mondiale.
{.is-success}


> En 1950, Turing publie un article fondateur o√π il pose une question r√©volutionnaire : **"Les machines peuvent-elles penser ?"**
> 
> Pour y r√©pondre, il propose le fameux **Test de Turing** (aussi appel√© "Jeu de l'imitation") :
> 
>    1. Un humain discute par √©crit avec deux interlocuteurs cach√©s
>    2. L'un est un humain, l'autre une machine
>    3. Si l'humain n'arrive pas √† distinguer la machine de l'humain, alors la machine "pense"
> 
> Ce test reste aujourd'hui une r√©f√©rence, m√™me si on sait qu'il a ses limites !
{.is-info}


Source : Test de turing


### 1956 : La Conf√©rence de Dartmouth

L'√©t√© 1956 marque un tournant historique. √Ä la **Conf√©rence de Dartmouth**, quatre pionniers (dont John McCarthy et Marvin Minsky) inventent officiellement le terme **"Intelligence Artificielle"**.

   Leur hypoth√®se de d√©part √©tait audacieuse :

   > *"Tout aspect de l'apprentissage ou de l'intelligence peut √™tre d√©crit si pr√©cis√©ment qu'une machine peut le simuler."*



### 1956 - 1974 : "Les ann√©es dor√©es"

C'est l'euphorie ! Les financements (notamment gouvernementaux) pleuvent, l'optimisme est √† son comble. En 1967, Marvin Minsky d√©clare m√™me :

> *"Dans une g√©n√©ration... le probl√®me de la cr√©ation de l'intelligence artificielle sera substantiellement r√©solu."*

> **Spoiler alert :** il se trompait.
{.is-danger}

Les chercheurs se concentrent sur des "micro-mondes" : des environnements simplifi√©s o√π l'IA peut raisonner sans la complexit√© du monde r√©el. C'est l'essor des premiers algorithmes de r√©solution de probl√®mes et du Traitement du Langage Naturel (NLP). Trois projets marquent cette √©poque :
   
   
  
#### ELIZA (1966) - Le premier chatbot

![ai.eliza.png](/ai_ml/ai.eliza.png){.align-center}

> ELIZA simulait un psychoth√©rapeute en reformulant simplement les phrases de l'utilisateur. Malgr√© sa simplicit√©, les gens √©taient bluff√©s et se confiaient √† "elle" !

#### SHRDLU et le Blocks World

![ai.blocks_world.jpg](/ai_ml/ai.blocks_world.jpg){.align-center}

> Un programme capable de manipuler des blocs virtuels en comprenant des ordres en langage naturel. "Pose le cube rouge sur le bloc bleu" ? Pas de probl√®me !

#### Shakey le robot (1972)

![ai.shakey_robot.jpg](/ai_ml/ai.shakey_robot.jpg){.align-center}

> Le premier robot mobile capable de percevoir son environnement et de planifier ses d√©placements. Un anc√™tre des robots aspirateurs d'aujourd'hui !


### 1974 - 1980 : "Hiver de l'IA" - La d√©sillusion

Apr√®s l'euphorie, la douche froide. Les financeurs r√©alisent que les promesses √©taient... comment dire... un peu exag√©r√©es.

   Le **Rapport Lighthill** (1973) ass√®ne le coup de gr√¢ce en Grande-Bretagne : les fonds sont coup√©s.

   **Pourquoi cet √©chec ?**

   Trois murs techniques ont stopp√© net les recherches :

   1. **Puissance de calcul insuffisante** : Les ordinateurs de l'√©poque √©taient tout simplement trop faibles pour les ambitions de l'IA.

   2. **L'explosion combinatoire** : R√©soudre un probl√®me simple, OK. Mais d√®s qu'on augmente la complexit√©, le temps de calcul explose de fa√ßon exponentielle. Impossible √† g√©rer !

   3. **Manque de donn√©es** : Pas de Big Data √† l'√©poque. Comment apprendre sans exemples ?

   En 1980, le philosophe **John Searle** enfonce le clou avec son exp√©rience de pens√©e de la **"Chambre Chinoise"** : il d√©montre qu'une machine peut donner les bonnes r√©ponses sans rien comprendre √† ce qu'elle dit. Elle ma√Ætrise la **syntaxe** mais pas la **s√©mantique**.

   > ‚ÑπÔ∏è **L'exp√©rience de la Chambre Chinoise**
   > 
   > Imagine-toi enferm√© dans une pi√®ce avec un livre de r√®gles. Des Chinois te passent des questions en chinois sous la porte. Tu suis les r√®gles du livre pour √©crire des r√©ponses en chinois. De l'ext√©rieur, tu sembles parler chinois... mais tu n'as rien compris ! Tu manipules des symboles sans en saisir le sens.


### Ann√©es 1980 : L'√®re des Syst√®mes Experts 


> L'IA sort des laboratoires (on parle d'IA symbolique) pour entrer dans les entreprises. C'est la premi√®re grande r√©ussite commerciale de la discipline gr√¢ce aux Syst√®mes Experts. Ces logiciels visent √† reproduire le raisonnement d'un sp√©cialiste humain (un m√©decin, un garagiste).
> 
> Leur fonctionnement est simple et repose sur deux piliers :
>
> - La Base de r√®gles (Le Savoir) : Une liste immense d'instructions de type "Si... Alors..." (Ex: Si le moteur chauffe, alors v√©rifier le radiateur).
>
> - Le Moteur d'inf√©rence (La Logique) : Le logiciel qui parcourt ces r√®gles pour trouver la solution √† un probl√®me donn√©.
>
> Note : En parall√®le, la recherche sur les r√©seaux de neurones (Deep Learning) commence discr√®tement √† rena√Ætre.
{.is-success}


### 1987 - 1993 : Le deuxi√®me "Hiver de l'IA" (La chute du Hardware)


> La bulle des Syst√®mes Experts √©clate. Le probl√®me ? Ces logiciels tournaient sur des ordinateurs ultra-co√ªteux et sp√©cialis√©s (les "Lisp Machines").
> 
> Soudain, les PC et les Mac deviennent assez puissants... et beaucoup moins chers. Pourquoi investir des fortunes dans du mat√©riel sp√©cialis√© ?
> 
> Nouvel hiver budg√©taire. Mais cette fois, quelque chose de crucial se met en place : la **d√©mocratisation des ordinateurs personnels**. Des millions de machines vont bient√¥t g√©n√©rer des tonnes de donn√©es...
{.is-success}


### 1993 - 2011 : Le triomphe de la Data (L'IA probabiliste)


Trois facteurs vont relancer la machine :

   **1. La Loi de Moore**
   La puissance de calcul double tous les 18 mois environ. Ce qui √©tait impossible hier devient trivial demain.

   **2. Le Big Data**
   Internet explose. Le smartphone arrive (2007). Soudain, on g√©n√®re des **quantit√©s astronomiques de donn√©es**.

   **3. L'apprentissage statistique**
   Changement de philosophie : on arr√™te d'essayer de coder des r√®gles parfaites. On laisse les algorithmes **apprendre des probabilit√©s** √† partir des donn√©es.

   L'IA devient enfin une discipline scientifique mature et rigoureuse.


### Aujourd'hui : L'√àre de l'Ubiquit√© et de l'√âthique

L'IA est partout. Dans ton t√©l√©phone, tes r√©seaux sociaux, ta banque, ta voiture...

La question n'est plus "Est-ce que √ßa marche ?" mais **"Est-ce que c'est juste ?"**

   > ‚ö†Ô∏è **Les nouveaux d√©fis**
   > 
   > - **Biais algorithmiques** : Une IA entra√Æn√©e sur des donn√©es biais√©es reproduit ces biais
   > - **Protection de la vie priv√©e** : Qui a acc√®s √† tes donn√©es ?
   > - **Manipulation de l'opinion** : Fake news, deepfakes...
   > - **Impact environnemental** : Entra√Æner un gros mod√®le consomme √©norm√©ment d'√©nergie

Comme le dit Brad Smith de Microsoft : la technologie touche d√©sormais aux droits humains fondamentaux. L'enjeu actuel est la **r√©gulation** et la cr√©ation d'une **IA √©thique et explicable**.

## Quickstart : L'IA en action


**Objectif** : Observer comment l'IA se "per√ßoit" et tester sa capacit√© √† structurer une r√©ponse complexe.

**Consigne** : Rends-toi sur une IA de ton choix (ChatGPT, Claude, Mistral, Gemini...) et entre le prompt suivant :

```text
Agis comme un formateur en IA expliquant les LLM √† un apprenant curieux. Structure ta r√©ponse en 4 couches :  
1. **Auto-description litt√©rale** : "Je suis un Grand Mod√®le de Langage (LLM), ce qui signifie..."  
2. **Analogie** : Compare un LLM √† une ¬´ **poup√©e russe de savoir humain compress√©** ¬ª (explique les symboles : couches de la poup√©e = couches du mod√®le, compression = entra√Ænement).  
3. **Auto-dissection** : D√©cris le *processus de g√©n√©ration* de *cette phrase m√™me* :  
   - Tokenisation ‚Üí Embeddings ‚Üí Attention ‚Üí Pr√©diction ‚Üí D√©codage  
4. **M√©tacognition** : "Pourquoi cette explication en couches fonctionne-t-elle ? Parce que les LLM apprennent le savoir *hi√©rarchiquement* ‚Äì refl√©tant la mani√®re dont je viens de vous l'enseigner."  
```

> üí° **Analyse tes r√©sultats**
> 
> Compare les r√©ponses de diff√©rentes IA. Tu remarqueras qu'elles sont capables de "m√©tacognition simul√©e" (expliquer leur propre fonctionnement). C'est une excellente d√©monstration de leurs capacit√©s de synth√®se !
> 
> Mais rappelle-toi : elles ne "comprennent" pas vraiment. Elles g√©n√®rent la suite de texte la plus probable. Nuance importante !
{.is-warning}



## Sous le capot : Les progr√®s techniques

Les LLM sont de plus en plus performants. Ce n'est pas de la magie, mais une convergence de plusieurs avanc√©es math√©matiques et techniques.

> Le rapport de Stanford sur l'IA (2025) d√©taille ces avanc√©es : [https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance)

### 1. La vectorisation du langages (Embeddings) (Word2Vec et al.)


Premier d√©fi : comment faire comprendre le langage humain √† une machine qui ne comprend que des chiffres ?

La r√©ponse : Les math√©matiques, **transformer les mots en vecteurs** (des listes de nombres).

![Word2Vec g√©n√©ral](/ai_ml/ai.fundamentals.word2vec.general.png){.align-center}


Gr√¢ce √† des techniques comme **Word2Vec**, chaque mot devient un point dans un espace math√©matique √† plusieurs dimensions.

![Vecteurs Word2Vec](/ai_ml/ai.fundamentals.word2vec.vectors.jpg){.align-center}


>  **Pourquoi c'est g√©nial ?**
> 
>    Dans cet espace, les mots qui ont des sens proches sont... g√©ographiquement proches ! On peut calculer des "distances" entre les mots :
> 
>    - "Roi" est proche de "Reine"
>    - "Paris" est √† "France" ce que "Rome" est √† "Italie"
> 
>    On peut m√™me faire des "calculs" avec les mots :
>    ```
>   Roi - Homme + Femme ‚âà Reine
>   ```
{.is-info}

> üí° **√Ä retenir**
> 
> Pour l'IA, le sens d'un mot est d√©fini par sa **position** dans cet espace math√©matique. Le mod√®le n'apprend pas juste des mots, il **cartographie des concepts**.
{.is-info}



### 2. Les r√©seaux neuronaux profonds (Deep Learning)

Une fois les mots transform√©s en chiffres, ils passent dans le "cerveau" du mod√®le : un **r√©seau de neurones artificiels**. On parle d'IA "Connexionistes".

![Taille des r√©seaux de neurones](/ai_ml/ai.fundamentals.nn-size.png)
*La taille des r√©seaux a explos√© ces derni√®res ann√©es !*

   Chaque neurone fait deux choses :
   1. Une **transformation affine** (transformation math√©matique) des entr√©es (combinaison lin√©aire pond√©r√©e)
   2. Une **fonction d'activation** qui introduit de la non-lin√©arit√© (comme ReLU ou Sigmo√Øde)

   C'est cette non-lin√©arit√© qui permet d'apprendre des choses complexes. Sans elle, empiler des couches ne servirait √† rien !

   > ‚ÑπÔ∏è **Pourquoi "profond" ?**
   > 
   > Plus le r√©seau a de couches (= plus il est "profond"), plus il peut comprendre des concepts abstraits. Les premi√®res couches d√©tectent des patterns simples, les derni√®res comprennent des notions complexes comme l'ironie ou le raisonnement logique.


### 3. L'attention et les Transformers (La r√©volution de 2017)

> En 2017, une √©quipe de Google publie un article au titre √©vocateur : **"Attention Is All You Need"** (L'attention, c'est tout ce qu'il vous faut).
> 
>    C'est LA r√©volution qui a rendu les LLM modernes possibles.
{.is-success}


   ![Sch√©ma LLM](/ai_ml/ai.fundamentals.llm-schema.png)

>    **Le probl√®me des anciennes approches**
> 
>    Avant les Transformers, les mod√®les traitaient le texte mot par mot, dans l'ordre. Pour une longue phrase, le mod√®le "oubliait" le d√©but quand il arrivait √† la fin.
{.is-info}


>    **La solution : le m√©canisme d'Attention**
> 
>    L'Attention permet au mod√®le de regarder **tous les mots en m√™me temps** et de d√©cider lesquels sont importants pour comprendre chaque partie.
> 
>    Prenons un exemple :
> 
>    > *"L'animal n'a pas travers√© la rue car **il** √©tait trop fatigu√©."*
> 
>    - Pour un humain, "il" d√©signe clairement "l'animal"
>    - Pour une ancienne IA, c'√©tait ambigu : la rue ? l'animal ?
>    - Gr√¢ce √† l'Attention, le mod√®le connecte fortement "il" √† "animal" et faiblement √† "rue"
> 
>    C'est ce m√©canisme qui permet de g√©n√©rer des textes longs et coh√©rents !
{.is-success}


### 4. Le renforcement par l'humain / Reinforcement Learning from Human Feedback (RLHF)

 √Ä ce stade, le mod√®le a √©t√© entra√Æn√© sur tout Internet. Il sait parler, mais... il peut √™tre impoli, raconter n'importe quoi, ou donner des conseils dangereux.

   Il faut l'**√©duquer**. C'est le r√¥le du **RLHF** (Reinforcement Learning from Human Feedback).

   ![Fine-tuning avec RLHF](/ai_ml/ai.fundamentals.fine-tuning-llm-with-rlhf.png)

   **Comment √ßa marche ?**

   1. Le mod√®le g√©n√®re plusieurs r√©ponses possibles
   2. Des humains classent ces r√©ponses (de la meilleure √† la pire)
   3. Le mod√®le ajuste ses param√®tres pour maximiser la "r√©compense" (la satisfaction humaine)

   C'est cette √©tape qui transforme un simple "compl√©teur de texte" en un **assistant conversationnel utile** comme ChatGPT ou Claude.

> üí° **C'est l'alignement !**
> 
> Le RLHF "aligne" le comportement du mod√®le sur les valeurs humaines. Sans cette √©tape, le mod√®le pourrait g√©n√©rer du contenu offensant ou dangereux.
{.is-warning}




## Panorama actuel des mod√®les (Juin 2025)

   ![Benchmark LLM Juin 2025](/ai_ml/ai.llm.benchmark.2025-06-04.png)


## Quelques mod√®les et entreprises du LLM

![ai-llm-benchmark-202501.png](/ai_ml/ai-llm-benchmark-202501.png)

En janvier 2025 :

 | Mod√®le                  | Entreprise | Type           |
   | ----------------------- | ---------- | -------------- |
   | **DeepSeek**            | DeepSeek   | Open Source üîì  |
   | **ChatGPT (GPT-4, o1)** | OpenAI     | Propri√©taire üîí |
   | **Llama**               | Meta       | Open Source üîì  |
   | **Claude**              | Anthropic  | Propri√©taire üîí |
   | **Qwen**                | Alibaba    | Open Weight üîì  |
   | **Codestral**           | Mistral    | Open Source üîì  |
   | **Gemini**              | Google     | Propri√©taire üîí |
   | **Gemma**               | Google     | Open Weight üîì  |

### O√π suivre les benchmarks ?

   Les performances des mod√®les √©voluent tr√®s vite ! Voici quelques ressources pour rester √† jour :

   - [Vellum LLM Leaderboard](https://www.vellum.ai/llm-leaderboard)
   - [Artificial Analysis](https://artificialanalysis.ai/leaderboards/models)
   - [BigCode Bench](https://bigcode-bench.github.io/)
   - [LLM Stats](https://llm-stats.com/)
   - [Aider Leaderboards](https://aider.chat/docs/leaderboards/) (pour le code)


## Comment choisir son mod√®le ? ü§î

   Face √† cette jungle de mod√®les, comment faire le bon choix ? Voici les crit√®res √† consid√©rer :

### Les crit√®res essentiels

   | Crit√®re              | Questions √† se poser                                               |
   | -------------------- | ------------------------------------------------------------------ |
   | **Efficacit√©**       | Le mod√®le est-il bon pour MA t√¢che ? (code, r√©daction, analyse...) |
   | **Co√ªts**            | Quel est le prix par token ? Mon budget le permet-il ?             |
   | **Open Source**      | Ai-je besoin d'acc√©der au code ? De le modifier ?                  |
   | **Ex√©cution locale** | Puis-je le faire tourner sur mes serveurs ?                        |
   | **Taille**           | Ai-je le mat√©riel pour faire tourner un gros mod√®le ?              |
   | **Confidentialit√©**  | Mes donn√©es sont-elles sensibles ?                                 |
   | **S√©curit√©**         | Le mod√®le a-t-il des garde-fous suffisants ?                       |
   | **Performance**      | Vitesse de r√©ponse acceptable ?                                    |

 ## QCM : Teste tes connaissances ! üß†

   **Question 1** : Quelle est la mission principale d'un LLM ?
   - A) Stocker des connaissances comme une encyclop√©die
   - B) Pr√©dire le mot suivant le plus probable
   - C) Comprendre le sens profond des textes
   - D) Remplacer les humains

   **Question 2** : Qu'est-ce que l'hallucination dans le contexte des LLM ?
   - A) Un bug logiciel
   - B) Le fait de g√©n√©rer des informations fausses avec assurance
   - C) Un probl√®me de connexion internet
   - D) Une fonctionnalit√© d√©sactiv√©e

   **Question 3** : Que permet le m√©canisme d'Attention ?
   - A) De rendre l'IA plus polie
   - B) De traiter tous les mots d'une phrase simultan√©ment
   - C) D'acc√©l√©rer les calculs
   - D) De r√©duire la consommation √©lectrique

   **Question 4** : √Ä quoi sert le RLHF ?
   - A) √Ä rendre le mod√®le plus rapide
   - B) √Ä aligner le comportement du mod√®le sur les attentes humaines
   - C) √Ä r√©duire la taille du mod√®le
   - D) √Ä traduire le mod√®le en fran√ßais

   <details>
   <summary>üìñ Voir les r√©ponses</summary>

   1. **B** - Pr√©dire le mot suivant le plus probable
   2. **B** - Le fait de g√©n√©rer des informations fausses avec assurance
   3. **B** - De traiter tous les mots d'une phrase simultan√©ment
   4. **B** - √Ä aligner le comportement du mod√®le sur les attentes humaines

   </details>


<!--
### Les interfaces

**Les mod√®les LLM sont disponibles sous forme de services API ou d'interfaces qui exploitent ces API.**

Si on prend l'exemple de Claude par Anthropic.

- Chat Web : [https://claude.ai/](https://claude.ai/) est un chatbot qui permet de tester Claude en ligne.
- API : [https://docs.anthropic.com/en/api/overview](https://docs.anthropic.com/en/api/overview) est la documentation de l'API Claude.
- SDK : [https://github.com/anthropics/claude-code-sdk-python](https://github.com/anthropics/claude-code-sdk-python) est un client Python pour l'API Claude.

Aujourd'hui, les mod√®les LLM sont int√©grables directement dans les applications, comme le font les IDE.

* * *

**Aujourd'hui on voit √©merger une vari√©t√© de solutions et d'interfaces.**

**A terme on aura de plus en plus d'architectures qui int√®greront l'AI en tant que service.**

- Endpoint d'inf√©rence : Code (autocompl√©tion) et Chat (questions / r√©ponses)
- Interface web
- Int√©gration IDE
- Int√©gration CLI
- Interfaces agentique
- Application IA

**En l'√©tat, toutes ces solutions utiliseront au final un prompt pour obtenir une r√©ponse d'un mod√®le de langage.**

-->