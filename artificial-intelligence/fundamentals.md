---
title: Les Fondamentaux
description: 
published: true
date: 2026-01-10T21:17:27.490Z
tags: 
editor: markdown
dateCreated: 2025-12-27T12:34:46.295Z
---

# Les Fondamentaux

> üéØ **Objectifs de ce chapitre**
> 
> √Ä la fin de ce chapitre, tu seras capable de :
> - Comprendre ce qu'est r√©ellement l'IA (et ce qu'elle n'est pas !)
> - Diff√©rencier IA, Machine Learning et Deep Learning
> - Expliquer comment fonctionne un LLM comme ChatGPT √† tes coll√®gues
> - Retracer les grandes √©tapes de l'histoire de l'IA
> - Choisir le bon mod√®le pour tes projets
   
![ai-ml-ds.png](/ai_ml/ai-ml-ds.png "Infographie par Jen Looper de Microsoft"){.align-center}
   
Tu as s√ªrement entendu parler d'Intelligence Artificielle partout ces derniers temps. √Ä la t√©l√©, sur les r√©seaux sociaux, au bureau... Tout le monde en parle, mais combien de personnes savent vraiment ce que c'est ?

   Bonne nouvelle : √† la fin de ce cours, **toi**, tu sauras. Et tu pourras m√™me l'expliquer √† ta grand-m√®re (enfin, si elle est curieuse üòÑ).


## Qu'est ce que l'IA ?


> Imaginons que tu veuilles cr√©er un robot qui range ta chambre. Tu pourrais √©crire des milliers de lignes de code pour chaque situation possible : "si chaussette par terre, alors ramasser et mettre dans panier". Fastidieux, non ?
> 
>    L'**Intelligence Artificielle**, c'est justement l'id√©e de cr√©er des programmes capables de **s'adapter** et de **r√©soudre des probl√®mes** sans qu'on ait besoin de tout leur expliquer dans les moindres d√©tails.
{.is-info}


> üí° **D√©finition simple**
> L'Intelligence Artificielle (IA), c'est la capacit√© d'une machine ou d'un programme √† imiter l'intelligence humaine pour r√©aliser des t√¢ches complexes.
{.is-success}


>  En gros, au lieu de dire √† l'ordinateur exactement quoi faire dans chaque cas, on lui donne les moyens d'**apprendre** et de **s'adapter**. C'est un changement de philosophie radical !
{.is-info}


## Qu'est ce que le Machine Learning ?

> OK, l'IA c'est bien joli, mais concr√®tement, comment on fait pour qu'une machine "apprenne" ?
> 
>    C'est l√† qu'entre en sc√®ne le **Machine Learning** (ou Apprentissage Automatique en bon fran√ßais).
{.is-info}


> üí° **D√©finition simple**
> Le **"Machine Learning"** (Apprentissage Automatique) est un sous-domaine de l'IA o√π la machine apprend √† r√©soudre des t√¢ches en analysant des donn√©es, plut√¥t qu'en suivant des r√®gles dict√©es ligne par ligne par un d√©veloppeur.
{.is-success}

### Une analogie pour bien comprendre

   Imagine que tu veuilles apprendre √† un enfant √† reconna√Ætre un chat. Tu as deux options :

   **Option 1 : La m√©thode classique (programmation traditionnelle)**
   Tu lui donnes une liste de r√®gles :
   - "Un chat a 4 pattes"
   - "Un chat a des moustaches"
   - "Un chat fait miaou"
   - "Un chat a une queue"
   - ...

   Le probl√®me ? Un chien aussi a 4 pattes et une queue. Et certains chats n'ont pas de queue (comme le Manx). Ta liste de r√®gles va vite devenir un cauchemar √† maintenir !

   **Option 2 : La m√©thode Machine Learning**
   Tu montres √† l'enfant des **milliers de photos de chats** en lui disant "√ßa, c'est un chat". √Ä force, son cerveau va naturellement rep√©rer les patterns qui font qu'un chat est un chat.

   Le Machine Learning, c'est exactement √ßa : on montre des **tonnes d'exemples** √† la machine, et elle apprend toute seule √† reconna√Ætre les patterns.

   > ‚ÑπÔ∏è **Bon √† savoir**
   > 
   > Le Machine Learning n√©cessite beaucoup de donn√©es pour fonctionner. C'est pour √ßa qu'on parle souvent de "Big Data" dans le m√™me souffle. Plus tu as de donn√©es de qualit√©, meilleur sera ton mod√®le !

## Qu'est ce que le Deep Learning ?

> üí° **D√©finition simple**
> Le **Deep Learning** (Apprentissage Profond) est une sous-cat√©gorie du Machine Learning qui s'inspire de la structure du cerveau humain. Il utilise des "r√©seaux de neurones artificiels" compos√©s de nombreuses couches superpos√©es pour apprendre des donn√©es tr√®s complexes (images, son, texte).
{.is-success}

### Pourquoi "Deep" (Profond) ?

   Le mot "profond" fait r√©f√©rence au nombre de **couches** dans le r√©seau de neurones. Plus il y a de couches, plus le r√©seau est "profond", et plus il peut apprendre des choses complexes.

   Pense √† une usine avec plusieurs √©tages :
   - **√âtage 1** : On d√©tecte des formes simples (lignes, courbes)
   - **√âtage 2** : On combine ces formes en motifs (yeux, oreilles)
   - **√âtage 3** : On assemble les motifs en concepts (visage de chat)
   - **√âtage 4** : On reconna√Æt l'objet final ("C'est F√©lix le chat !")

   Chaque couche apprend quelque chose de plus abstrait que la pr√©c√©dente. C'est cette profondeur qui permet au Deep Learning de comprendre des choses aussi complexes que le langage humain ou la reconnaissance faciale.


## Qu'est ce qu'un LLM ?

   Maintenant qu'on a pos√© les bases, parlons de ce qui fait le buzz : les **LLM** (Large Language Models, ou Grands Mod√®les de Langage).

   ChatGPT, Claude, Gemini, Mistral... Tous ces assistants IA que tu utilises peut-√™tre d√©j√† sont des LLM.

> üí° **D√©finition simple**
> Un **LLM** (Large Language Model) est un mod√®le de langage capable de g√©n√©rer du texte √† partir d'un prompt donn√©. Il est bas√© sur du Deep Learning et entra√Æn√© sur une immense partie d'Internet, avec pour objectif principal de pr√©dire le mot suivant le plus probable dans une phrase.
{.is-success}

### Attends... "Pr√©dire le mot suivant" ? C'est tout ?

Eh oui ! Aussi impressionnants qu'ils paraissent, les LLM sont fondamentalement des **machines √† pr√©dire le mot suivant**. C'est leur seule mission dans la vie.

Mais attention, ne sous-estime pas cette t√¢che apparemment simple. Pour pr√©dire correctement le mot suivant, le mod√®le doit comprendre :
   - La grammaire
   - Le contexte
   - Les concepts
   - Les relations logiques
   - Et bien plus encore...


### Comment √ßa marche concr√®tement ?
![ai.fundamentals.token-guessing.jpg](/ai_ml/ai.fundamentals.token-guessing.jpg){.align-center}




![ai.fundamentals.tokenization.jpg](/ai_ml/ai.fundamentals.tokenization.jpg){.align-center}

> Le Mod√®le ne voit pas vraiment des "mots". Il voit des s√©quences de **tokens** (des morceaux de mots). Mais pour simplifier, restons avec les mots.
{.is-success}


Prenons l'exemple du Prompt : 
> Le chat mange ‚Üí [Le], [chat], [mange]

Le mod√®le calcule les probabilit√©s du mot suivant (Token) :
   | Mot suivant | Probabilit√© |
   |-------------|-------------|
   | des croquettes | 42% ‚úÖ |
   | une souris | 25% |
   | du poisson | 15% |
   | sa p√¢t√©e | 10% |
   | une pizza | 0.01% üçï |

Le mod√®le choisit g√©n√©ralement le mot le plus probable ("des croquettes"). Et ensuite ? Il recommence ! Il prend "Le chat mange des croquettes" et pr√©dit le mot suivant. Et ainsi de suite... c'est ce que l'on appelle l'**Auto-r√©gression**.

> ‚ö†Ô∏è **Attention !**
> 
> Un LLM est une **machine statistique**, pas une encyclop√©die. Il cherche le "probable", pas le "vrai". C'est pour √ßa qu'il peut parfois inventer des informations avec un aplomb d√©concertant !
> 
> Ce ph√©nom√®ne s'appelle l'**hallucination**. Le mod√®le ne "sait" pas qu'il dit des b√™tises. Il g√©n√®re simplement la suite de texte la plus probable selon ses calculs. Si cette suite probable est fausse... tant pis pour lui (et pour toi si tu ne v√©rifies pas) !
{.is-warning}

>  Tu as peut-√™tre entendu parler de la **temp√©rature** dans le contexte des LLM. C'est un param√®tre qui contr√¥le le niveau de "prise de risque" du mod√®le.

   | Temp√©rature | Comportement |
   |-------------|--------------|
   | **0** (basse) | Le mod√®le choisit toujours le mot le plus probable. R√©ponses pr√©visibles et "s√ªres". |
   | **0.7-0.8** (moyenne) | Un bon √©quilibre entre cr√©ativit√© et coh√©rence. |
   | **1.0+** (haute) | Le mod√®le prend des risques, peut choisir des mots moins probables. Plus cr√©atif, mais aussi plus susceptible de partir dans tous les sens ! |

Pour reprendre notre exemple du chat :
- Si la temp√©rature est √† 0, l'IA choisira toujours "des croquettes" (le plus probable).
- Si la temp√©rature est √©lev√©e (ex: 0.8), l'IA prendra des risques et pourra choisir "sa p√¢t√©e" ou m√™me "le canap√©" pour √™tre plus cr√©ative.


    
## Histoire de l'IA

> L'IA n'est pas n√©e avec ChatGPT en 2022. Son histoire remonte √† plus de 70 ans, avec des hauts spectaculaires et des bas tout aussi impressionnants.
{.is-info}


![ai.history_1.png](/ai_ml/ai.history_1.png)
![ai.history_2.png](/ai_ml/ai.history_2.png)

### 1950 : Des machines qui pensent

Tout commence avec un g√©nie britannique : **Alan Turing**.
> 
> üí° **Qui est Alan Turing ?**
> 
> √âlu scientifique du 20√®me si√®cle en 2019, Alan Turing est consid√©r√© comme le p√®re de l'IA et de l'informatique th√©orique. Il a formalis√© le concept d'algorithme avec sa "Machine de Turing" (1936) et jou√© un r√¥le crucial dans le d√©cryptage de la machine Enigma pendant la Seconde Guerre Mondiale.
{.is-success}


> En 1950, Turing publie un article fondateur o√π il pose une question r√©volutionnaire : **"Les machines peuvent-elles penser ?"**
> 
> Pour y r√©pondre, il propose le fameux **Test de Turing** (aussi appel√© "Jeu de l'imitation") :
> 
>    1. Un humain discute par √©crit avec deux interlocuteurs cach√©s
>    2. L'un est un humain, l'autre une machine
>    3. Si l'humain n'arrive pas √† distinguer la machine de l'humain, alors la machine "pense"
> 
> Ce test reste aujourd'hui une r√©f√©rence, m√™me si on sait qu'il a ses limites !
{.is-info}


Source : Test de turing


### 1956 : La Conf√©rence de Dartmouth

L'√©t√© 1956 marque un tournant historique. √Ä la **Conf√©rence de Dartmouth**, quatre pionniers (dont John McCarthy et Marvin Minsky) inventent officiellement le terme **"Intelligence Artificielle"**.

   Leur hypoth√®se de d√©part √©tait audacieuse :

   > *"Tout aspect de l'apprentissage ou de l'intelligence peut √™tre d√©crit si pr√©cis√©ment qu'une machine peut le simuler."*



### 1956 - 1974 : "Les ann√©es dor√©es"

C'est l'euphorie ! Les financements (notamment gouvernementaux) pleuvent, l'optimisme est √† son comble. En 1967, Marvin Minsky d√©clare m√™me :

> *"Dans une g√©n√©ration... le probl√®me de la cr√©ation de l'intelligence artificielle sera substantiellement r√©solu."*

> **Spoiler alert :** il se trompait.
{.is-danger}

Les chercheurs se concentrent sur des "micro-mondes" : des environnements simplifi√©s o√π l'IA peut raisonner sans la complexit√© du monde r√©el. C'est l'essor des premiers algorithmes de r√©solution de probl√®mes et du Traitement du Langage Naturel (NLP). Trois projets marquent cette √©poque :
   
   
  
#### ELIZA (1966) - Le premier chatbot

![ai.eliza.png](/ai_ml/ai.eliza.png){.align-center}

> ELIZA simulait un psychoth√©rapeute en reformulant simplement les phrases de l'utilisateur. Malgr√© sa simplicit√©, les gens √©taient bluff√©s et se confiaient √† "elle" !

#### SHRDLU et le Blocks World

![ai.blocks_world.jpg](/ai_ml/ai.blocks_world.jpg){.align-center}

> Un programme capable de manipuler des blocs virtuels en comprenant des ordres en langage naturel. "Pose le cube rouge sur le bloc bleu" ? Pas de probl√®me !

#### Shakey le robot (1972)

![ai.shakey_robot.jpg](/ai_ml/ai.shakey_robot.jpg){.align-center}

> Le premier robot mobile capable de percevoir son environnement et de planifier ses d√©placements. Un anc√™tre des robots aspirateurs d'aujourd'hui !


### 1974 - 1980 : "Hiver de l'IA" - La d√©sillusion

> Apr√®s l'euphorie, la d√©sillusion. Au milieu des ann√©es 70, les financeurs (comme la DARPA) r√©alisent que les promesses de l'IA ont √©t√© exag√©r√©es. C'est le d√©but de l'Hiver de l'IA : une p√©riode de coupes budg√©taires drastiques et de scepticisme, d√©clench√©e notamment par le critique Rapport Lighthill (1973).
> 
> Trois murs techniques ont stopp√© les recherches :
>
> - Limitations : La puissance de calcul √©tait trop limit√©e. Le mat√©riel n'√©tait tout simplement pas pr√™t pour traiter les informations n√©cessaires √† une "vraie" intelligence.
>      
> - L'explosion combinatoire : Les chercheurs d√©couvrent que r√©soudre des probl√®mes simples ne suffit pas. D√®s qu'on augmente la complexit√©, le temps de calcul augmente de fa√ßon exponentielle, d√©passant les capacit√©s des ordinateurs de l'√©poque.
> 
> - Manques de donn√©es: Pas assez de donn√©es, ce qui entravait le processus de test, de d√©veloppement et de raffinement des algorithmes.
>     
> - Critiques philosophiques : En 1980, John Searle contre-attaque le Test de Turing avec l'exp√©rience de la "Chambre Chinoise". Il prouve q'une machine peut donner les bonnes r√©ponses sans rien comprendre √† ce qu'elle dit. Elle maitrise la syntaxe mais pas la s√©mantique.
> 
> Enfin, une guerre interne divise la communaut√© : les "Scruffies" (bricoleurs de code intuitif, comme pour SHRDLU) s'opposent aux "Neats" (partisans de la logique formelle et rigoureuse). C'est finalement l'approche "Neat" qui l'emportera pour garantir des r√©sultats explicables et scientifiques.
{.is-success}


### Ann√©es 1980 : L'√®re des Syst√®mes Experts 


> L'IA sort des laboratoires pour entrer dans les entreprises. C'est la premi√®re grande r√©ussite commerciale de la discipline gr√¢ce aux Syst√®mes Experts. Ces logiciels visent √† reproduire le raisonnement d'un sp√©cialiste humain (un m√©decin, un garagiste).
> 
> Leur fonctionnement est simple et repose sur deux piliers :
>
> - La Base de r√®gles (Le Savoir) : Une liste immense d'instructions de type "Si... Alors..." (Ex: Si le moteur chauffe, alors v√©rifier le radiateur).
>
> - Le Moteur d'inf√©rence (La Logique) : Le logiciel qui parcourt ces r√®gles pour trouver la solution √† un probl√®me donn√©.
>
> Note : En parall√®le, la recherche sur les r√©seaux de neurones (Deep Learning) commence discr√®tement √† rena√Ætre.
{.is-success}


### 1987 - 1993 : Le deuxi√®me "Hiver de l'IA" (La chute du Hardware)

> La crise des machines sp√©cialis√©es La bulle des Syst√®mes Experts √©clate √† la fin des ann√©es 80. Le probl√®me est √©conomique : ces logiciels tournaient sur des ordinateurs tr√®s co√ªteux et sp√©cialis√©s (comme les "Lisp Machines").
>
> Soudainement, les ordinateurs personnels (PC et Apple), bien moins chers, deviennent assez puissants pour les concurrencer. Les entreprises abandonnent les solutions sp√©cialis√©es co√ªteuses. C'est un nouvel hiver budg√©taire, mais qui pr√©pare le terrain : la d√©mocratisation des PC va bient√¥t permettre de collecter des donn√©es partout.
{.is-success}

### 1993 - 2011 : Le triomphe de la Data (L'IA probabiliste)

> La convergence : Puissance de calcul + Big Data Cette p√©riode marque le retour en gr√¢ce de l'IA, port√©e par trois facteurs techniques qui r√©solvent les blocages pass√©s :
>
> La Loi de Moore : La puissance de calcul explose, permettant enfin de traiter des algorithmes lourds.
>
> Le Big Data : Avec internet et l'arriv√©e du smartphone (2007), la quantit√© de donn√©es disponibles devient gigantesque.
>
> L'apprentissage statistique : On arr√™te d'essayer de coder des r√®gles logiques parfaites. On laisse les algorithmes "deviner" et apprendre des probabilit√©s √† partir de ces immenses bases de donn√©es. L'IA devient une discipline scientifique mature et rigoureuse.
{.is-success}

Cette √©poque a vu une nouvelle √®re pour l'apprentissage automatique et l'IA, permettant de r√©soudre certains des probl√®mes caus√©s auparavant par le manque de donn√©es et de puissance de calcul. La quantit√© de donn√©es a commenc√© √† augmenter rapidement et √† devenir plus largement disponible, pour le meilleur et pour le pire, notamment avec l'av√®nement du smartphone vers 2007. La puissance de calcul a augment√© de mani√®re exponentielle, et les algorithmes ont √©volu√© en parall√®le. Le domaine a commenc√© √† gagner en maturit√© √† mesure que les jours libres du pass√© se cristallisaient en une v√©ritable discipline.

### Aujourd'hui : L'√àre de l'Ubiquit√© et de l'√âthique

> Grands pouvoirs, grandes responsabilit√©s L'IA est d√©sormais invisible mais omnipr√©sente (smartphones, r√©seaux sociaux, banques). La question n'est plus "Est-ce que √ßa marche ?", mais "Est-ce que c'est juste ?".
>
> Cette √®re est marqu√©e par une prise de conscience des risques soci√©taux : biais algorithmiques, protection de la vie priv√©e et manipulation de l'opinion. Comme le souligne Brad Smith (Microsoft), la technologie touche d√©sormais aux droits humains fondamentaux. L'enjeu actuel est donc la r√©gulation et la cr√©ation d'une IA √©thique et explicable.

## ÔøΩQuickstart : L'IA en action

Maintenant que vous en savez plus sur la th√©orie, passons √† la pratique. Prenez une IA (ChatGPT, Claude, Mistral, Gemini...) et entrez le prompt suivant.

Objectif : Observer comment l'IA se "per√ßoit" et tester sa capacit√© √† structurer une r√©ponse complexe.

```text
Agis comme un formateur en IA expliquant les LLM √† un apprenant curieux. Structure ta r√©ponse en 4 couches :  
1. **Auto-description litt√©rale** : "Je suis un Grand Mod√®le de Langage (LLM), ce qui signifie..."  
2. **Analogie** : Compare un LLM √† une ¬´ **poup√©e russe de savoir humain compress√©** ¬ª (explique les symboles : couches de la poup√©e = couches du mod√®le, compression = entra√Ænement).  
3. **Auto-dissection** : D√©cris le *processus de g√©n√©ration* de *cette phrase m√™me* :  
   - Tokenisation ‚Üí Embeddings ‚Üí Attention ‚Üí Pr√©diction ‚Üí D√©codage  
4. **M√©tacognition** : "Pourquoi cette explication en couches fonctionne-t-elle ? Parce que les LLM apprennent le savoir *hi√©rarchiquement* ‚Äì refl√©tant la mani√®re dont je viens de vous l'enseigner."  
```

> Analyse : Comparez les r√©ponses. Vous remarquerez que l'IA est capable de "m√©tacognition simul√©e" (expliquer son propre fonctionnement), ce qui est une excellente d√©monstration de ses capacit√©s de synth√®se.
{.is-warning}



## Sous le capot : Les progr√®s techniques

Les LLM sont de plus en plus performants. Ce n'est pas de la magie, mais une convergence de plusieurs avanc√©es math√©matiques et techniques.

Cf. [https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance)


### La vectorisation du langages (Embeddings) (Word2Vec et al.)

La premi√®re √©tape pour qu'une machine comprenne le langage, c'est de transformer les mots en math√©matiques.

La vectorisation (ou Word2Vec et successeurs) permet de repr√©senter les mots par des vecteurs num√©riques dans un espace multidimensionnel.

![ai.fundamentals.word2vec.general.png](/ai_ml/ai.fundamentals.word2vec.general.png){.align-center}
![ai.fundamentals.word2vec.vectors.jpg](/ai_ml/ai.fundamentals.word2vec.vectors.jpg){.align-center}

Cette capacit√© permet de calculer des distances entre les mots :

    Le mot "Roi" est math√©matiquement proche de "Reine".

    Le mot "Paris" est proche de "France" de la m√™me mani√®re que "Rome" est proche de "Italie".

Au fur et √† mesure de son entra√Ænement, le mod√®le n'apprend pas juste des mots, il cartographie des concepts.

    √Ä retenir : Pour l'IA, le sens d'un mot est d√©fini par sa position dans cet espace math√©matique.


### Les r√©seaux neuronaux profonds (Deep Learning)

**La taille des IA dites "Connexionistes" a grandement √©volu√©e depuis quelques ann√©es.**

![ai.fundamentals.nn-size.png](/ai_ml/ai.fundamentals.nn-size.png){.align-center}

Une fois les mots transform√©s en chiffres, ils passent dans le "cerveau" du mod√®le.

Chaque neurone artificiel applique une transformation affine (combinaison lin√©aire pond√©r√©e des entr√©es) suivie d'une fonction d'activation non lin√©aire (comme ReLU ou Sigmo√Øde).

L'empilement de ces couches (d'o√π le terme "Deep" Learning) permet d'approximer des fonctions extr√™mement complexes. Plus le r√©seau est profond, plus il est capable de comprendre des nuances subtiles et des abstractions (ironie, code complexe, raisonnement logique).



### L'attention et les transformers

**Le concept d'attention a permis des progr√®s consid√©rables dans le traitement du langage naturel via la prise en compte des s√©quences de mots et phrases au lieu de mot √† mot*.*

![ai.fundamentals.llm-schema.png](/ai_ml/ai.fundamentals.llm-schema.png)

Le m√©canisme d'Attention permet au mod√®le de pond√©rer dynamiquement l'importance des √©l√©ments d'une s√©quence.

C'est un m√©canisme qui permet de calculer des poids d'importance pour chaque √©l√©ment d'une s√©quence lors du traitement d'une cible.

C'est ainsi que les LLM peuvent produire des phrases coh√©rentes avec le contexte.

Prenons la phrase : "L'animal n'a pas travers√© la rue car il √©tait trop fatigu√©."

    Pour un humain, "il" d√©signe "l'animal".

    Pour une ancienne IA, c'√©tait ambigu (la rue ? l'animal ?).

    Gr√¢ce √† l'Attention, le mod√®le connecte fortement le mot "il" au mot "animal" et faiblement au mot "rue". C'est ce qui permet de g√©n√©rer des textes longs et coh√©rents.

### Le renforcement par l'humain / Reinforcement Learning from Human Feedback (RLHF)

Une fois le mod√®le entra√Æn√© sur tout Internet, il sait parler, mais il peut √™tre grossier, raciste ou inutile. Il faut l'√©duquer.

**Pour cela, la derni√®re √©tape consiste √† rendre le mod√®le plus performant en lui fournissant des r√©compenses pour les actions appropri√©es et des punitions pour les actions inappropri√©es.**

![ai.fundamentals.fine-tuning-llm-with-rlhf.png](/ai_ml/ai.fundamentals.fine-tuning-llm-with-rlhf.png)



Le RLHF est donc l'√©tape d'**alignement** du comportement du mod√®le. On r√©compense le mod√®le pour les bonnes r√©ponses et on le "punit" pour les mauvaises.

1. Le mod√®le g√©n√®re plusieurs r√©ponses.

2. Des humains classent ces r√©ponses de la meilleure √† la pire.

3. Le mod√®le ajuste ses param√®tres pour maximiser la "r√©compense" (satisfaction humaine).


Cette derni√®re √©tape optimise le r√©seau neuronal brut via une approche de fine-tuning et transforme un "compl√©teur de texte" brut en un "assistant conversationnel" utile (comme ChatGPT).



## Panorama actuel des mod√®les (Juin 2025)

![ai.llm.benchmark.2025-06-04.png](/ai_ml/ai.llm.benchmark.2025-06-04.png)

Il existe des benchmarks automatis√©s :

- [https://www.vellum.ai/llm-leaderboard](https://www.vellum.ai/llm-leaderboard)
- [https://artificialanalysis.ai/leaderboards/models](https://artificialanalysis.ai/leaderboards/models)
- [https://bigcode-bench.github.io/](https://bigcode-bench.github.io/)
- [https://llm-stats.com/](https://llm-stats.com/)
- [https://aider.chat/docs/leaderboards/](https://aider.chat/docs/leaderboards/)


## Quelques mod√®les et entreprises du LLM

![ai-llm-benchmark-202501.png](/ai_ml/ai-llm-benchmark-202501.png)

En janvier 2025 :

- DeepSeek / DeepSeek (Open Source)
- ChatGPT / OpenAI (Propri√©taire)
- Llama / Meta (Open Source)
- Claude / Anthropic (Propri√©taire)
- Qwen / Alibaba (Open Weight / Source)
- Codestral / Mistral (Open Source)
- Gemini / Google (Propri√©taire)
- Gemma / Google (Open Weight)


## Les contraintes de choix

**Comment choisir un mod√®le adapt√© √† ses besoins ?**

Il y a diff√©rents param√®tres √† consid√©rer dans l'absolu :

- Efficacit√© pour les t√¢ches demand√©es
- Co√ªts
- Open Source
- Ex√©cution locale
- Taille
- Privacy
- S√©curit√©
- Performance

Souvent, les d√©veloppeurs sont d√©pendants des mod√®les disponibles dans leur organisation.

**Il est n√©anmoins utile d'avoir une id√©e des mod√®les disponibles et de leur efficacit√©, si l'on consid√®re qu'il s'agit d'outils de d√©veloppement essentiels.**


<!--
### Les interfaces

**Les mod√®les LLM sont disponibles sous forme de services API ou d'interfaces qui exploitent ces API.**

Si on prend l'exemple de Claude par Anthropic.

- Chat Web : [https://claude.ai/](https://claude.ai/) est un chatbot qui permet de tester Claude en ligne.
- API : [https://docs.anthropic.com/en/api/overview](https://docs.anthropic.com/en/api/overview) est la documentation de l'API Claude.
- SDK : [https://github.com/anthropics/claude-code-sdk-python](https://github.com/anthropics/claude-code-sdk-python) est un client Python pour l'API Claude.

Aujourd'hui, les mod√®les LLM sont int√©grables directement dans les applications, comme le font les IDE.

* * *

**Aujourd'hui on voit √©merger une vari√©t√© de solutions et d'interfaces.**

**A terme on aura de plus en plus d'architectures qui int√®greront l'AI en tant que service.**

- Endpoint d'inf√©rence : Code (autocompl√©tion) et Chat (questions / r√©ponses)
- Interface web
- Int√©gration IDE
- Int√©gration CLI
- Interfaces agentique
- Application IA

**En l'√©tat, toutes ces solutions utiliseront au final un prompt pour obtenir une r√©ponse d'un mod√®le de langage.**

-->